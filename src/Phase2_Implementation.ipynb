{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNnxO5ydec1n"
      },
      "source": [
        "<h1>\n",
        "<font color=\"#FA8072\">Install Requirements</font>\n",
        "</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "7sAPfxCpeRfE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48a891f3-4231-4771-e5a4-13077a94563b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: simpletransformers in /usr/local/lib/python3.10/dist-packages (0.63.11)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.47.0 in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (4.65.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (2022.10.31)\n",
            "Requirement already satisfied: transformers>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (4.30.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (2.13.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (1.2.2)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (1.2.2)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (2.12.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (1.5.3)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (0.13.3)\n",
            "Requirement already satisfied: wandb>=0.10.32 in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (0.15.5)\n",
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (1.24.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from simpletransformers) (0.1.99)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers>=4.6.0->simpletransformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.6.0->simpletransformers) (0.16.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.6.0->simpletransformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.6.0->simpletransformers) (6.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.6.0->simpletransformers) (0.3.1)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (8.1.4)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (3.1.32)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (1.28.1)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (0.4.0)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (0.1.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (1.3.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.10.32->simpletransformers) (3.20.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->simpletransformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->simpletransformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->simpletransformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->simpletransformers) (3.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->simpletransformers) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->simpletransformers) (0.3.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->simpletransformers) (3.2.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->simpletransformers) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets->simpletransformers) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->simpletransformers) (3.8.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->simpletransformers) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->simpletransformers) (2022.7.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->simpletransformers) (1.3.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->simpletransformers) (3.1.0)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (1.6.2)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (5.3.1)\n",
            "Requirement already satisfied: importlib-metadata<7,>=1.4 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (6.8.0)\n",
            "Requirement already satisfied: pillow<10,>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (8.4.0)\n",
            "Requirement already satisfied: pympler<2,>=0.9 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (1.0.1)\n",
            "Requirement already satisfied: rich<14,>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (13.4.2)\n",
            "Requirement already satisfied: tenacity<9,>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (8.2.2)\n",
            "Requirement already satisfied: toml<2 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (4.7.1)\n",
            "Requirement already satisfied: tzlocal<5,>=1.1 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (4.3.1)\n",
            "Requirement already satisfied: validators<1,>=0.2 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (0.20.0)\n",
            "Requirement already satisfied: pydeck<1,>=0.1.dev5 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (0.8.1b0)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (6.3.1)\n",
            "Requirement already satisfied: watchdog in /usr/local/lib/python3.10/dist-packages (from streamlit->simpletransformers) (3.0.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (1.56.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (2.3.6)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard->simpletransformers) (0.40.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->simpletransformers) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->simpletransformers) (3.1.2)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->simpletransformers) (4.3.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit->simpletransformers) (0.12.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.10.32->simpletransformers) (1.16.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->simpletransformers) (1.3.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb>=0.10.32->simpletransformers) (4.0.10)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard->simpletransformers) (1.3.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7,>=1.4->streamlit->simpletransformers) (3.15.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.11.0->streamlit->simpletransformers) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.11.0->streamlit->simpletransformers) (2.14.0)\n",
            "Requirement already satisfied: pytz-deprecation-shim in /usr/local/lib/python3.10/dist-packages (from tzlocal<5,>=1.1->streamlit->simpletransformers) (0.1.0.post0)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from validators<1,>=0.2->streamlit->simpletransformers) (4.4.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->simpletransformers) (2.1.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb>=0.10.32->simpletransformers) (5.0.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit->simpletransformers) (0.19.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.11.0->streamlit->simpletransformers) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->simpletransformers) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->simpletransformers) (3.2.2)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.10/dist-packages (from pytz-deprecation-shim->tzlocal<5,>=1.1->streamlit->simpletransformers) (2023.3)\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.27.8\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece\n",
        "!pip install simpletransformers\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxYQ0SpTeIal"
      },
      "source": [
        "<h1>\n",
        "<font color=\"#FA8072\">Mount Drive</font>\n",
        "</h1>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FT5YCQCNeRnW",
        "outputId": "2e889fb5-66d8-4dd4-a654-f53a5abc832c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kK67Nz6eZp8"
      },
      "source": [
        "<h1>\n",
        "<font color=\"#FA8072\">Import Libraries</font>\n",
        "</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Xuj0gK-fMYR"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "u2KrxYvEeY8k"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import sentencepiece as spm\n",
        "import shutil\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import openai\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report\n",
        "from transformers import TFDistilBertModel, DistilBertTokenizer\n",
        "from tensorflow.keras.layers import Dense, Input, Conv1D, GlobalMaxPooling1D, LSTM\n",
        "from tensorflow.keras.models import Model\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from gensim.models import KeyedVectors\n",
        "from simpletransformers.language_modeling import LanguageModelingModel\n",
        "from sklearn.model_selection import KFold\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-O_A5MDiY5T4",
        "outputId": "d37f6052-8531-4be9-af5c-a03cbf24e0e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "nltk.download(\"punkt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCS5RZKRVUv5"
      },
      "source": [
        "<h1>\n",
        "<font color=\"#FA8072\">1. Word2Vec</font>\n",
        "</h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4gOh5jqevoI"
      },
      "source": [
        "<h2>\n",
        "<font color=\"#FA8072\">1.1 Train Word2Vec Models for Each Data Category\n",
        "</font>\n",
        "</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "nS5Xj95jmR1H"
      },
      "outputs": [],
      "source": [
        "categories = [\"true\", \"mostly-true\", \"half-true\", \"barely-true\", \"false\", \"pants-fire\"]\n",
        "data_dir = \"/content/gdrive/MyDrive\"\n",
        "clean_data_dir = os.path.join(data_dir, \"data/clean/\")\n",
        "wordbroken_dir = os.path.join(data_dir, \"data/wordbroken/\")\n",
        "sentenceborken = os.path.join(data_dir, \"data/sentencebroken/\")\n",
        "models_dir = os.path.join(data_dir, \"models/word2vec/\")\n",
        "tokenization_dir = os.path.join(data_dir, \"models/tokenization/\")\n",
        "language_dir = os.path.join(data_dir, \"models/language/\")\n",
        "architecture_dir = os.path.join(data_dir, \"models/architecture/\")\n",
        "stats_dir = os.path.join(data_dir, \"stats/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "dwwIvWbunW6T"
      },
      "outputs": [],
      "source": [
        "for category in categories:\n",
        "    category_sentence_dir = os.path.join(sentenceborken, f\"{category}-sentences.csv\")\n",
        "    model_save_path = f\"{models_dir}{category}.word2vec.npy\"\n",
        "    sentencebroken_data = pd.read_csv(category_sentence_dir)\n",
        "    sentences = [nltk.word_tokenize(sentence) for sentence in sentencebroken_data['Statement']]\n",
        "    model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "    model.wv.save(model_save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x13qi0DCVsbJ"
      },
      "source": [
        "<h2>\n",
        "<font color=\"#FA8072\">1.2 Compare and Analyze Vectors of Common Words</font>\n",
        "</h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lMc4I9oV2_V",
        "outputId": "d562e481-472b-45fc-9449-5c2eda859049"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Common words with the same vectors:\n",
            "['increase', 'men', 'Democrats', 'great', 'part', 'drug', 'home', 'state', 'including', 'things', 'done', 'Under', 'if', 'me', 'Hillary', 'Jim', 'won', '7', 'give', 'between', 'Parenthood', 'better', 'their', 'After', 'An', 'taxes', 'ID', 'County', 'girls', 'mayor', '5', 'I', 'poll', 'due', 'wants', 'she', 'family', 'Americas', 'youre', 'Mexico', 'came', 'around', 'bill', 'closed', 'these', '30', 'long', ',', 'Says', 'know', 'Medicare', 'everyone', 'Ohio', 'check', 'so', '60', 'cases', 'went', 'wouldnt', 'over', 'reduce', 'just', 'George', 'includes', 'attack', 'its', 'income', 'Act', 'money', 'study', 'nation', 'World', '6', 'votes', 'line', 'water', 'born', 'started', 'New', 'how', 'international', 'after', 'name', 'risk', 'using', 'proposed', 'gay', 'it', 'giving', '3', 'there', 'covered', 'think', 'Jersey', 'crime', 'thats', 'like', 'nearly', 'without', 'are', 'police', 'always', 'plan', 'lost', 'much', 'Carolina', 'We', 'time', '10', 'leader', 'the', 'to', '100', 'individuals', '200', 'reported', 'before', 'held', 'Trump', 'from', 'Marco', 'behind', 'officers', 'gun', 'across', 'But', 'holding', 'This', 'too', 'mass', 'according', 'same', 'get', 'free', 'relief', 'and', 'pay', 'charge', 'United', 'workers', 'San', 'kids', 'sent', 'him', 'many', 'stopped', 'hit', 'away', 'economic', 'In', 'taken', 'in', '2020', '13', 'believe', 'gives', 'car', 'massive', 'personal', 'Ukraine', 'education', 'far', 'world', 'average', 'off', 'failed', 'ended', 'economy', 'testing', 'deficit', 'Paul', 'had', 'Planned', 'man', 'former', 'Chris', 'Day', 'buy', 'Last', 'countries', 'To', 'Joe', 'families', 'among', 'ban', 'Earth', 'data', 'tests', 'cut', 'levels', 'got', 'gets', 'current', 'polling', 'approved', 'domestic', 'caught', 'criminal', 'when', 'race', 'he', 'want', 'let', 'as', 'photo', 'security', 'Obamas', 'As', 'his', 'zero', 'rights', 'House', 'three', 'U.S.', 'percent', 'crimes', 'during', 'children', 'tax', 'could', 'weeks', 'paid', '4', 'refused', 'two', 'rates', 'help', 'high', 'enough', 'Rubio', '...', 'against', 'North', 'There', 'Robert', '2000', 'not', 'Wisconsin', '2016', 'Republican', \"''\", 'close', 'being', 'work', 'fraud', 'benefits', 'abortions', 'come', 'public', 'through', 'attacks', 'that', 'credit', 'most', 'insurance', 'keep', '20', 'political', 'said', 'case', 'major', 'officials', 'nuclear', 'we', 'aid', 'no', 'debt', 'top', 'day', 'didnt', 'seven', 'good', 'Gov', 'left', 'of', 'Rep.', 'isnt', 'spent', 'support', 'Party', 'Ron', 'Texas', 'change', 'down', 'executive', 'jobs', 'system', '400', 'alone', 'saying', 'given', 'National', 'private', 'new', 'single', 'called', 'cost', 'were', 'Brown', 'Social', 'working', 'at', 'John', 'Of', 'James', 'population', 'U.S', 'other', 'assault', 'banned', 'until', 'wanted', 'die', 'news', 'themselves', 'issues', 'asked', 'federal', 'illegally', 'pass', 'young', 'must', ']', 'used', 'into', 'save', 'defund', 'about', 'one', 'your', 'while', 'able', 'makes', '80', 'became', 'say', 'citizens', 'can', 'twice', 'killed', 'states', 'West', 'report', 'again', 'president', 'And', 'years', 'black', 'administration', 'put', 'sending', 'Americans', 'actually', 'Department', 'theyre', 'government', 'thousands', 'bad', 'dont', 'brought', 'our', 'abortion', 'than', 'paying', 'out', 'job', 'prices', 'taking', 'spend', 'immigrants', 'women', 'local', 'medical', 'Health', 'full', 'lead', 'receive', 'kill', 'some', 'or', 'Austin', 'debate', 'General', 'continue', 'supports', '12', 'wrote', 'signed', 'COVID19', 'total', 'going', '75', 'each', 'Obama', 'victims', 'Justice', 'One', 'pays', 'What', 'Russia', 'still', 'every', 'making', 'prevent', 'a', 'rape', 'early', 'policies', 'care', 'they', '``', 'lot', 'already', 'should', 'secret', 'open', 'Theres', 'for', 'almost', 'cant', '100000', 'hasnt', 'laws', 'second', 'on', 'talking', 'city', 'today', 'food', 'then', 'service', 'company', 'Security', 'Were', 'bought', 'theres', 'terrorists', 'nothing', 'hes', 'Medicaid', 'employees', 'five', 'use', 'highest', 'taxpayer', 'turned', 'ballots', 'has', 'middle', 'own', 'coverage', 'birth', 'Rick', 'low', 'likely', 'very', '15', 'big', 'Ted', 'allows', 'Chicago', 'increases', 'doesnt', 'ago', 'took', 'national', 'When', 'Supreme', 'No', 'next', 'real', 'who', 'crisis', 'white', 'Israel', 'policy', 'endorsed', 'include', 'Illinois', '$', 'helped', 'since', 'carry', 'voted', 'but', 'student', 'Congress', 'be', 'released', 'health', 'special', 'changed', 'elected', 'dollars', 'see', 'take', 'climate', 'days', 'So', 'The', 'wrong', 'Virginia', 'near', 'voter', 'safe', 'Romney', 'violence', 'enforcement', '500', 'President', 'spread', 'Biden', 'Not', 'building', 'by', 'Nancy', 'been', 'history', 'parts', 'war', 'made', 'you', 'goes', 'live', 'That', 'raise', 'energy', 'more', 'preexisting', 'allow', 'illegal', 'cities', '70', 'America', 'force', 'run', 'what', 'getting', 'even', 'States', 'Court', 'way', '25', 'also', 'having', 'South', 'forced', 'drop', 'small', 'funding', 'It', 'life', 'received', 'within', 'January', 'law', 'doing', 'receiving', 'have', 'companies', 'Sen.', 'Democratic', 'campaign', 'this', 'African', 'year', 'Donald', 'Scott', 'elections', 'California', 'really', 'up', 'voters', 'China', 'legal', 'million', 'death', 'them', 'rate', 'times', 'says', 'compared', 'Over', 'Barack', '1', 'people', 'does', 'violent', 'passed', 'IRS', 'You', 'candidate', 'vote', 'millions', 'find', 'board', 'gas', 'Program', 'wall', 'would', 'seen', 'gave', 'tried', 'A', 'weapon', 'general', 'order', 'do', 'fully', 'hospital', 'Im', 'Trumps', '1000', 'schools', 'longer', 'call', 'trying', 'well', 'emergency', '90', 'make', 'office', 'last', 'Iran', '14', 'All', 'veterans', 'allowed', 'set', 'group', 'person', 'was', 'number', 'arrest', 'less', 'test', 'Clinton', 'never', 'presidential', 'State', 'age', 'largest', 'Georgia', 'may', 'both', 'shows', 'told', 'power', 'right', 'recently', 'doctors', 'end', 'They', 'become', 'because', 'per', 'Its', 'where', 'school', 'died', 'increased', 'any', 'patients', 'GOP', 'sexual', 'capital', 'military', 'only', 'here', 'election', 'At', 'is', 'More', 'Bill', 'York', '2', 'those', 'prison', 'go', 'all', 'my', 'fact', 'place', 'look', 'back', 'billion', 'few', 'City', 'months', 'which', 'found', 'numbers', 'woman', 'Black', 'baby', 'someone', 'unemployment', 'FBI', 'show', 'coming', 'leaders', 'Secretary', 'voting', 'coronavirus', 'recent', 'did', 'members', 'control', 'child', 'now', 'deaths', 'provide', 'David', 'country', 'deal', 'month', 'first', 'calls', 'need', 'under', 'Washington', 'wont', 'Democrat', 'If', 'past', 'arrested', 'border', 'Office', '[', 'American', 'War', 'hours', 'dropped', 'four', 'flag', 'us', 'cancer', 'created', 'Cruz', '11', 'with', 'Florida', 'worker', 'create', 'students', 'Johnson', 'White', 'Senate', 'an', 'will', 'entire', 'seniors', 'ever', '.', 'budget', 'Republicans', 'For', 'program', 'oil', 'her']\n",
            "Common words with different vectors:\n",
            "['half', 'Anthony', 'annual', 'stamps', 'adopted', 'buying', 'Mississippi', 'NRA', 'leadership', 'comes', 'jail', 'house', 'earn', 'lawmakers', 'changes', 'ordered', 'surge', 'moon', 'Obamacare', 'assistance', 'prosecutor', '26', 'CEO', 'drink', 'admitted', 'racial', 'Four', 'another', 'murder', 'Japan', 'demand', 'Georgias', 'v.', 'double', 'hard', 'marijuana', 'sanctions', 'With', 'attendance', 'Matter', 'immigrant', 'evidence', 'victim', '21', 'attorney', 'Union', 'biggest', '2006', 'Force', 'response', 'treatment', 'worked', 'May', 'grade', 'clear', 'effectively', 'condemned', 'perhaps', 'reform', 'handguns', 'third', 'talk', 'simply', 'expensive', 'Italy', 'basically', 'tied', 'area', '6000', 'regulations', 'facing', 'Ronald', 'purge', 'key', 'Fox', 'eligible', '10000', 'flying', 'type', 'Plan', 'Even', 'rules', 'nine', 'April', 'sure', 'Committee', 'loses', 'Barrett', 'results', 'forces', 'raising', 'drugs', 'difference', 'vice', 'fighting', 'Many', 'CDC', 'Miami', '300000', 'smoking', 'K12', 'rally', 'cuts', 'benefit', 'Great', 'courts', 'stand', 'drivers', 'requires', 'He', 'Putin', 'roof', 'Because', 'following', 'Arizona', 'standards', 'Perrys', 'My', 'vacation', 'longterm', 'leading', 'England', 'lawyers', 'salaries', 'Our', 'bridge', 'Crist', 'Guard', '33', 'plus', 'hand', '68', 'nominee', 'Center', 'career', 'Kathy', '2010', 'Air', 'Vegas', 'video', 'mental', 'training', 'effect', 'false', 'industry', 'waste', '2014', 'mother', 'Carolinas', 'creation', 'closing', 'throughout', 'sex', 'Between', 'happened', 'step', 'Virginias', 'dog', 'miles', 'interview', 'Afghanistan', 'wind', 'Taliban', 'legally', 'possession', 'apply', 'Deal', 'represent', '42', '250000', 'later', 'October', 'TV', 'gang', 'farm', 'questions', 'Homeland', 'Ive', 'everybody', 'corporate', 'sold', 'permanent', 'positions', 'fly', 'promoting', '5000', 'expansion', 'vaccine', 'politicians', 'housing', 'signs', 'departments', 'yet', 'something', 'civil', 'seeing', 'least', 'hundreds', 'costing', 'II', 'pregnant', 'requests', 'ISIS', 'stood', 'gross', 'recipients', 'himself', 'carried', 'sentenced', 'planned', 'gubernatorial', 'connection', 'room', 'direct', 'inside', 'chief', 'manufacturing', 'residents', 'senator', 'online', 'Control', 'community', 'forward', 'According', 'takes', 'beat', 'Canada', 'criminals', 'Economic', 'Civil', 'Foxconn', 'multiple', 'agree', 'game', 'mortgage', 'During', 'Russian', 'secure', 'Steve', 'pandemic', 'parents', 'Council', 'concealed', 'sick', 'card', 'theyve', 'running', 'greatest', 'cars', 'Mexican', 'color', 'decrease', 'enter', 'friends', 'cutting', 'agency', 'effort', 'stop', 'Houston', 'staff', '85', 'face', 'Police', 'financial', 'resulted', 'blocked', 'wait', 'defense', 'leave', 'firearm', 'learn', 'Your', 'committed', '36', 'virtually', 'owned', 'meant', 'armed', 'reality', '50000', 'Francisco', 'Lake', '35', 'Wade', 'Europe', 'hiring', 'fuel', 'homes', 'Abraham', 'project', 'politician', 'Katie', 'College', 'lawsuit', 'Beto', 'anything', 'fall', 'class', 'property', 'led', 'Well', 'modern', 'firearms', 'proven', 'By', 'Harry', 'sector', 'reelection', 'outside', 'air', 'others', 'wear', 'aliens', '97', '4000', 'website', 'Warner', 'firm', 'bailout', 'Madison', 'soldiers', 'proposing', 'Dems', 'Michael', 'Two', '1.5', 'huge', 'heart', 'movement', 'terrorist', 'Nearly', 'failing', 'collection', 'doubled', 'sell', 'Eric', 'often', 'trillion', 'gotten', 'slash', 'address', 'cap', 'land', 'Government', 'judge', 'minority', 'fracking', 'hands', '63', 'presidents', 'soccer', 'senators', 'banks', 'currently', 'NFL', 'bringing', 'August', 'human', 'supported', 'Army', 'Carter', 'Keystone', 'homeless', 'revenue', 'owns', 'Independence', 'agents', 'package', 'opponent', 'Nobody', 'ruled', 'investigation', 'goal', 'action', '16', 'returned', 'undocumented', '1968', 'feet', 'Reed', 'phone', 'Voting', 'cause', 'problem', 'panels', 'despite', 'European', 'Indiana', '2015', 'Martin', 'suffered', 'code', 'officer', '....', 'prior', 'allowing', 'once', 'rising', 'Colorado', 'hospitals', 'babies', 'walk', 'streets', 'adults', 'mailin', 'taxpayers', 'turn', 'flu', 'decades', 'listed', 'ICE', 'constitutional', 'Street', 'spoke', 'grocery', 'Why', 'charity', 'served', 'formula', 'machines', 'Bruce', 'saw', '24', 'approve', 'either', 'summer', 'floor', 'These', '1960s', 'bring', 'Bureau', 'counties', 'Rhode', 'Philadelphia', 'chance', 'director', '22', 'murdered', 'medicine', 'Lee', 'University', 'suspects', 'stadium', 'Women', 'rolls', 'Wisconsins', 'hurt', 'resolution', 'calling', 'games', 'kept', 'social', 'D.C.', 'thing', 'Reagan', 'Antonio', 'Congressional', 'anybody', 'involved', 'System', '15000', 'actions', 'science', 'Say', 'break', 'exist', 'Pelosi', 'tell', 'stimulus', 'gone', 'Milwaukee', 'lots', 'kills', 'supposed', 'model', 'governments', 'rent', 'remains', '50', 'weve', 'point', 'nomination', 'reduced', 'parole', 'wasnt', 'subject', 'amount', 'fire', 'Governor', 'trust', 'produce', 'putting', 'prevents', 'book', 'showed', 'identification', 'active', 'experience', 'Oz', 'cover', 'Twitter', 'Latinos', 'Schools', 'charges', 'ones', 'driving', 'fouryear', 'spreading', 'hate', 'infrastructure', 'folks', 'Alabama', 'cents', 'eliminate', 'whether', 'else', 'sea', 'Education', 'grant', '2013', 'decisions', 'harassment', 'legislation', 'lying', 'Jack', 'party', 'added', 'grid', 'crashed', 'figure', 'night', 'quarter', 'plans', 'greenhouse', 'access', 'Kelly', 'Arabia', 'programs', 'product', 'whole', 'spy', 'designed', '27', 'probably', 'bans', 'why', 'Portland', 'license', 'actual', 'killer', 'Mike', 'commit', 'players', 'build', 'available', 'threatened', 'rise', 'points', 'costs', 'session', 'significant', '39', 'natural', 'Evers', 'wife', 'creating', 'Most', 'rid', 'previous', 'stands', 'worth', 'younger', 'universal', '95', 'healthcare', 'female', 'needs', 'cast', 'stuck', 'mean', 'offices', 'impact', 'Boston', 'Michelle', 'refuse', 'hired', 'team', 'conditions', 'reach', 'reports', 'Seven', 'fund', 'While', 'pipeline', 'McConnell', 'citizen', 'fill', 'best', 'seek', 'peoples', 'send', 'Hurricane', 'moment', 'database', 'Al', 'authority', 'imposed', 'Thats', 'college', 'businesses', 'DeSantis', 'collecting', 'outofstate', 'alcohol', '8', 'different', 'agenda', 'Korea', 'womens', 'toxic', 'arent', 'Roe', 'named', 'drilling', 'overseas', 'bank', 'ground', 'saved', 'profits', 'communities', 'issue', 'protect', '45', 'global', 'mine', '300', 'Mary', 'raises', 'causing', 'supporting', 'East', 'along', 'Bible', 'absentee', '19', 'adding', 'everything', 'Paris', 'street', 'level', 'limit', 'dies', 'Chuck', 'Josh', 'train', 'trade', 'violation', 'Roy', 'cannabis', 'areas', 'Ill', 'Weve', 'absolutely', 'choice', 'hike', 'safety', 'throw', 'account', 'warrant', 'rating', 'responsible', 'vehicle', 'daughter', 'Lives', '40000', 'havent', 'radical', 'Kim', 'operations', 'met', 'supply', 'serving', 'carbon', 'Walmart', 'Nevada', 'Tim', 'remove', 'learning', 'built', 'prisoners', 'recession', 'inflation', 'claims', 'Kansas', 'specifically', 'audit', 'business', 'COVID', 'theft', 'together', 'funds', 'known', 'speech', 'form', 'Defense', 'shootings', 'helping', 'Her', 'choose', 'upon', 'stay', 'record', 'dramatically', 'accidents', 'slavery', 'gains', 'invested', 'ahead', 'anyone', 'donations', 'related', 'sense', 'organizations', 'lowest', 'football', 'FEMA', 'conservative', 'question', 'start', 'generated', 'teachers', 'solar', 'Tom', 'prohibited', 'Mitt', 'developing', 'illness', 'threat', 'position', 'status', 'matter', 'hour', 'written', 'girl', 'worst', 'tons', 'Then', 'restrictions', 'Due', 'Children', '30000', 'statement', 'speak', 'Energy', 'poor', 'deny', 'happen', 'lose', 'background', 'sports', 'showing', 'Amendment', 'hold', '31', '20000', 'immigration', 'annually', 'Mark', 'lives', 'Bidens', 'nationwide', 'subsidies', 'groceries', 'losing', 'humans', 'Every', 'week', 'prosecuted', 'She', 'release', '29', 'records', 'St.', 'clean', 'warming', 'electric', 'profit', 'southern', 'groups', 'womans', 'directly', 'Mayor', '2008', 'suggested', 'employers', 'protection', 'Protection', 'paper', 'governor', 'shutdown', 'travel', 'stock', 'nations', 'meet', 'Park', 'Syria', 'earned', 'requiring', 'ALL', 'pregnancy', 'dioxide', 'lower', 'quickly', 'First', 'Missouri', 'accused', 'share', 'win', 'deadly', 'Oregon', 'doctor', 'transgender', 'overall', 'approval', 'rich', 'operating', 'worlds', 'stores', 'stage', 'announced', 'words', 'result', 'Board', 'equipment', 'required', 'meeting', 'mandatory', 'Gates', 'looking', 'act', 'red', 'ages', 'amnesty', 'lawsuits', 'Hassan', 'needed', 'November', 'serious', 'guns', 'heard', '52', '600', 'oppose', 'decision', 'university', 'From', 'salary', 'drunk', 'Election', 'Theyve', 'storm', 'similar', 'denied', 'trafficking', 'Youre', 'am', '1.2', 'USA', 'Federal', 'El', 'several', 'centers', 'Disease', 'US', 'email', 'sale', 'On', 'scientists', 'remain', 'thousand', 'straight', '911', 'France', 'growth', 'masks', 'blood', 'controls', 'truck', 'district', 'estate', 'block', 'proposal', 'milk', 'lowered', 'Do', 'July', 'front', 'migrants', 'removed', 'production', 'Three', 'donated', 'meetings', 'normal', 'based', 'means', 'super', 'present', 'collapse', 'department', '2021', 'troops', 'Foundation', 'Spanish', 'event', 'secretly', '150', 'Iraq', 'targeted', 'Before', 'little', 'track', 'Constitution', 'problems', 'justice', 'NATO', 'mandates', 'Green', 'side', 'spending', 'source', 'dangerous', 'gallon', 'fired', 'Dr.', 'murderers', 'Back', 'cell', 'advertising', '2018', 'literally', 'handing', 'finally', 'emails', 'signing', 'ways', 'Capitol', 'Iowa', 'majority', 'seeking', 'Middle', 'supplies', 'visa', 'collect', '56', '28', 'sitting', 'Russians', 'require', 'positive', 'March', 'identify', 'electricity', 'agriculture', 'entered', 'Soviet', 'Lincoln', 'fight', 'Agriculture', 'Navy', 'totally', 'Theyre', 'Judge', 'sign', 'dying', 'mail', 'polls', 'Today', 'politics', 'checks', 'possible', 'vaccinated', 'claimed', 'Bush', 'products', 'agencies', '18', 'member', 'bus', 'People', 'dime', 'planning', 'HIV', 'eliminated', 'basis', 'threats', 'promised', 'soon', 'Service', 'town', 'governors', 'count', 'overdose', 'market', 'implement', 'Jon', 'fellow', 'cash', 'GDP', 'stated', 'read', 'information', 'Cuomo', 'return', 'Now', 'ran', 'space', 'charged', 'vaccination', 'Warren', 'head', '2017', 'suicide', 'Big', 'India', 'midterm', 'Tony', 'introduced', 'Half', 'kind', 'moved', 'true', 'opposed', 'Andrew', 'net', 'ballot', 'drive', 'old', 'presidency', 'shes', 'airport', 'center', 'strike', 'providing', 'orientation', 'trial', '..', 'contains', 'changing', 'large', 'strip', 'bipartisan', 'supporters', 'caused', 'teenage', 'period', 'living', 'married', 'though', 'labor', 'list', 'Census', 'contract', 'move', 'mask', 'Former', 'broke', 'broken', 'licenses', 'traffic', 'course', 'language', 'Since', 'Eisenhower', 'moving', 'favor', 'permit', 'union', 'Buffalo', 'affordable', 'Jeff', 'court', 'elementary', 'shot', 'June', 'term', 'Pelosis', 'household', 'developed', 'secretary', 'tens', 'School', 'common', 'wealth', 'Saudi', 'dollar', 'Schumer', 'larger', 'teach', 'ability', 'invented', 'minors', '2019', 'eight', '40', '9', 'Convention', 'plant', 'idea', 'provided', 'beginning', 'couple', 'Facebook', 'welfare', 'loss', 'knows', 'dead', 'Charlie', 'Wall', 'recorded', 'price', 'diversity', 'shown', 'Amazon', 'six', 'completely', 'Bay', 'committee', 'About', 'Californias', 'terrorism', 'Atlanta', 'hearing', 'Michigan', 'registered', 'attacked', 'weapons', 'raised', '200000', 'Coast', 'such', 'hundred', 'Island', 'ending', 'DACA', 'Governors', 'facilities', 'custody', '17', 'environmental', 'McDonalds', 'might', 'employee', 'Only', 'issued', 'cameras', 'press', 'mandate', 'Just', 'media', 'primary', 'minimum', 'reason', 'Attorney', 'Walker', 'candidates', 'tested', 'Harris', 'declared', 'included', 'higher', 'Mitch', 'foreign', 'funded', '700', 'fought', 'hole', 'century', 'flat', 'maybe', 'auto', 'experts', 'condition', 'leaving', 'requirement', 'waiting', 'add', 'Gun', 'planet', 'felons', 'confirmed']\n"
          ]
        }
      ],
      "source": [
        "category_models = {}\n",
        "for category in categories:\n",
        "    category_model_path = os.path.join(models_dir, f\"{category}.word2vec.npy\")\n",
        "    if os.path.exists(category_model_path):\n",
        "        category_model = KeyedVectors.load(category_model_path)\n",
        "        category_models[category] = category_model\n",
        "\n",
        "common_words = set.intersection(*[set(category_model.index_to_key) for category_model in category_models.values()])\n",
        "\n",
        "same_vectors = []\n",
        "different_vectors = []\n",
        "for word in common_words:\n",
        "    vectors = [category_models[category].get_vector(word) for category in category_models]\n",
        "    similarity_scores = cosine_similarity(vectors)\n",
        "    if np.all(similarity_scores >= 0.8):\n",
        "        same_vectors.append(word)\n",
        "    else:\n",
        "        different_vectors.append(word)\n",
        "\n",
        "print(\"Common words with the same vectors:\")\n",
        "print(same_vectors)\n",
        "\n",
        "print(\"Common words with different vectors:\")\n",
        "print(different_vectors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8oSwvz6TD0H"
      },
      "source": [
        "<h2>\n",
        "<font color=\"#FA8072\">1.3 Train Word2Vec Model on All Data</font>\n",
        "</h2>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCDaE75in_j9",
        "outputId": "69d5737f-0ecc-4207-9727-ab614837a8aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model for all labels saved at /content/gdrive/MyDrive/models/word2vec/all.word2vec.npy\n"
          ]
        }
      ],
      "source": [
        "all_model_save_path = os.path.join(models_dir, \"all.word2vec.npy\")\n",
        "\n",
        "combined_sentences = []\n",
        "for category in categories:\n",
        "    category_sentence_dir = os.path.join(sentenceborken, f\"{category}-sentences.csv\")\n",
        "    if os.path.exists(category_sentence_dir):\n",
        "        category_sentence_data = pd.read_csv(category_sentence_dir)\n",
        "        combined_sentences.extend(category_sentence_data['Statement'])\n",
        "\n",
        "tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in combined_sentences]\n",
        "\n",
        "model_combined = Word2Vec(tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "model_combined.wv.save(all_model_save_path)\n",
        "\n",
        "print(\"Model for all labels saved at\", all_model_save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>\n",
        "<font color=\"#FA8072\">2. Tokenization</font>\n",
        "</h1>"
      ],
      "metadata": {
        "id": "F_uGnCz_RoNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_sizes = [100, 500, 1000, 5000]"
      ],
      "metadata": {
        "id": "uw5vlJGIU-tv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(data, n_splits=5):\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "    return list(kf.split(data))"
      ],
      "metadata": {
        "id": "K7Yn7esDRtkE"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate_tokenizer(sentences, tokenizer_size):\n",
        "    tokenizer_results = []\n",
        "    data_splits = split_data(sentences)\n",
        "    for size in tokenizer_size:\n",
        "        unk_percentages = []\n",
        "        for i, (train_index, eval_index) in enumerate(data_splits):\n",
        "            train_data = [sentences[idx] for idx in train_index]\n",
        "            eval_data = [sentences[idx] for idx in eval_index]\n",
        "            train_data_file = f\"train_data_{size}_{i}.txt\"\n",
        "            with open(train_data_file, \"w\") as f:\n",
        "                f.write(\"\\n\".join(train_data))\n",
        "            model_prefix = f\"tokenizer_{size}_{i}\"\n",
        "            spm.SentencePieceTrainer.train(\n",
        "                f\"--input={train_data_file} --model_prefix={model_prefix} --vocab_size={size} --hard_vocab_limit=false\")\n",
        "            tokenizer = spm.SentencePieceProcessor()\n",
        "            tokenizer.load(f\"{model_prefix}.model\")\n",
        "            total_tokens = 0\n",
        "            unk_tokens = 0\n",
        "            for text in eval_data:\n",
        "                tokens = tokenizer.encode_as_pieces(text)\n",
        "                total_tokens += len(tokens)\n",
        "                unk_tokens += tokens.count(\"▁UNK\")\n",
        "            unk_percentage = (unk_tokens / total_tokens) * 100 if total_tokens != 0 else 0\n",
        "            unk_percentages.append(unk_percentage)\n",
        "        tokenizer_results.append({\"Size\": size, \"Unk Percentages\": sum(unk_percentages) / len(unk_percentages)})\n",
        "    return tokenizer_results"
      ],
      "metadata": {
        "id": "uDdauHE9VBIs"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_data = {}\n",
        "for category in categories:\n",
        "    category_csv = os.path.join(clean_data_dir, f\"{category}-cleaned.csv\")\n",
        "    if os.path.exists(category_csv):\n",
        "        cleaned_data[category] = pd.read_csv(category_csv)\n",
        "combined_sentences = []\n",
        "for category in categories:\n",
        "    category_sentence_dir = os.path.join(sentenceborken, f\"{category}-sentences.csv\")\n",
        "    if os.path.exists(category_sentence_dir):\n",
        "        category_sentence_data = pd.read_csv(category_sentence_dir)\n",
        "        combined_sentences.extend(category_sentence_data['Statement'])\n",
        "tokenizer_results = train_and_evaluate_tokenizer(combined_sentences, tokenizer_sizes)\n",
        "results_df = pd.DataFrame(tokenizer_results)\n",
        "results_path = os.path.join(stats_dir, \"tokenizer_results.csv\")\n",
        "results_df.to_csv(results_path, index=False)\n",
        "mean_percentages = [np.mean(percentages) for percentages in results_df[\"Unk Percentages\"]]\n",
        "best_size = results_df[\"Size\"].iloc[np.argmin(mean_percentages)]\n",
        "print(\"Best Tokenizer Size:\", best_size)\n",
        "best_model_prefix = f\"tokenizer_{best_size}_0\"\n",
        "best_tokenizer_path = os.path.join(tokenization_dir, f\"best_tokenizer_{best_size}.model\")\n",
        "os.makedirs(os.path.dirname(best_tokenizer_path), exist_ok=True)\n",
        "shutil.move(f\"{best_model_prefix}.model\", best_tokenizer_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "XgrGinarVbIw",
        "outputId": "332e1577-11e2-4a0b-bc1a-a08171210132"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Tokenizer Size: 100\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/MyDrive/models/tokenization/best_tokenizer_100.model'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>\n",
        "<font color=\"#FA8072\">3. Language Model</font>\n",
        "</h1>"
      ],
      "metadata": {
        "id": "5mAFNHBRkm2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "language_models = {\n",
        "    \"true\": \"gpt2\",\n",
        "    \"mostly-true\": \"gpt2\",\n",
        "    \"half-true\": \"gpt2\",\n",
        "    \"barely-true\": \"gpt2\",\n",
        "    \"false\": \"gpt2\",\n",
        "    \"pants-fire\": \"gpt2\"\n",
        "}"
      ],
      "metadata": {
        "id": "uZB_dwT0oCpC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for category in categories:\n",
        "    clean_data_file = os.path.join(clean_data_dir, f\"{category}-cleaned.csv\")\n",
        "    df = pd.read_csv(clean_data_file)\n",
        "    language_model_save_path = os.path.join(language_dir, f\"{category}.language_model\")\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(language_models[category])\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "    model = GPT2LMHeadModel.from_pretrained(language_models[category])\n",
        "    tokenized_data = tokenizer(df[\"Statement\"].tolist(), truncation=True, padding=True, return_tensors=\"pt\")\n",
        "    model.train()\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "    model.forward = model.generate\n",
        "    model.save_pretrained(language_model_save_path)\n",
        "    print(f\"Language model for category '{category}' saved at {language_model_save_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVT1UtIOWwDS",
        "outputId": "e89f334d-453a-4782-df05-20650aa4eb7b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Language model for category 'true' saved at /content/gdrive/MyDrive/models/language/true.language_model\n",
            "Language model for category 'mostly-true' saved at /content/gdrive/MyDrive/models/language/mostly-true.language_model\n",
            "Language model for category 'half-true' saved at /content/gdrive/MyDrive/models/language/half-true.language_model\n",
            "Language model for category 'barely-true' saved at /content/gdrive/MyDrive/models/language/barely-true.language_model\n",
            "Language model for category 'false' saved at /content/gdrive/MyDrive/models/language/false.language_model\n",
            "Language model for category 'pants-fire' saved at /content/gdrive/MyDrive/models/language/pants-fire.language_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_sentences = []\n",
        "for category in categories:\n",
        "    generated_sentences = []\n",
        "    for _ in range(5):\n",
        "        input_ids = tokenizer.encode(\"Generate a sentence\", return_tensors=\"pt\")\n",
        "        outputs = model.generate(input_ids, max_length=100, num_return_sequences=1)\n",
        "        generated_sentence = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        generated_sentences.append(generated_sentence)\n",
        "    output_file = os.path.join(stats_dir, \"txts\", f\"{category}_sentences.txt\")\n",
        "    with open(output_file, \"w\") as f:\n",
        "        for sentence in generated_sentences:\n",
        "            f.write(sentence + \"\\n\")\n",
        "    print(f\"Generated sentences for category '{category}' saved at {output_file}\")"
      ],
      "metadata": {
        "id": "l7bRKzGnk9eT",
        "outputId": "7f8b089d-90db-4a6e-f0a3-4335cba8118e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated sentences for category 'true' saved at /content/gdrive/MyDrive/stats/txts/true_sentences.txt\n",
            "Generated sentences for category 'mostly-true' saved at /content/gdrive/MyDrive/stats/txts/mostly-true_sentences.txt\n",
            "Generated sentences for category 'half-true' saved at /content/gdrive/MyDrive/stats/txts/half-true_sentences.txt\n",
            "Generated sentences for category 'barely-true' saved at /content/gdrive/MyDrive/stats/txts/barely-true_sentences.txt\n",
            "Generated sentences for category 'false' saved at /content/gdrive/MyDrive/stats/txts/false_sentences.txt\n",
            "Generated sentences for category 'pants-fire' saved at /content/gdrive/MyDrive/stats/txts/pants-fire_sentences.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>\n",
        "<font color=\"#FA8072\">4. Feature Engineering</font>\n",
        "</h1>"
      ],
      "metadata": {
        "id": "HIibl7LwLdqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_length_classification(X_train, X_test, y_train, y_test):\n",
        "    X_train = [[len(sentence)] for sentence in X_train]\n",
        "    X_test = [[len(sentence)] for sentence in X_test]\n",
        "    model = LogisticRegression()\n",
        "    model.fit(X_train, y_train)\n",
        "    train_acc = accuracy_score(y_train, model.predict(X_train))\n",
        "    test_acc = accuracy_score(y_test, model.predict(X_test))\n",
        "    return train_acc, test_acc"
      ],
      "metadata": {
        "id": "aRukinrQsMyI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word_length_classification(X_train, X_test, y_train, y_test):\n",
        "    X_train = [[len(word) for word in sentence.split()] for sentence in X_train]\n",
        "    X_test = [[len(word) for word in sentence.split()] for sentence in X_test]\n",
        "    X_train = [length for sublist in X_train for length in sublist]\n",
        "    X_test = [length for sublist in X_test for length in sublist]\n",
        "    model = DecisionTreeClassifier()\n",
        "    model.fit(np.array(X_train).reshape(-1, 1), y_train)\n",
        "    train_acc = accuracy_score(y_train, model.predict(np.array(X_train).reshape(-1, 1)))\n",
        "    test_acc = accuracy_score(y_test, model.predict(np.array(X_test).reshape(-1, 1)))\n",
        "    return train_acc, test_acc"
      ],
      "metadata": {
        "id": "0mgTRGfCPmUx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = {}\n",
        "for category in categories:\n",
        "    category_csv = os.path.join(clean_data_dir, f\"{category}-cleaned.csv\")\n",
        "    if os.path.exists(category_csv):\n",
        "        data[category] = pd.read_csv(category_csv)\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
        "train_accs = []\n",
        "val_accs = []\n",
        "test_accs = []\n",
        "for feature in ['sentence_length', 'word_length']:\n",
        "    train_acc, val_acc, test_acc = train_and_evaluate_tokenizer(train_data, val_data, test_data, feature)\n",
        "    train_accs.append(train_acc)\n",
        "    val_accs.append(val_acc)\n",
        "    test_accs.append(test_acc)\n",
        "epochs = range(1, len(train_accs) + 1)\n",
        "plt.plot(epochs, train_accs, label='Train')\n",
        "plt.plot(epochs, val_accs, label='Validation')\n",
        "plt.plot(epochs, test_accs, label='Test')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Classification Accuracy by Feature')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plot_path = os.path.join(stats_dir, \"classification_accuracy.png\")\n",
        "plt.savefig(plot_path)\n",
        "plt.close()\n",
        "print(\"Classification accuracy plot saved at\", plot_path)"
      ],
      "metadata": {
        "id": "FR8sElrfPuLf",
        "outputId": "457639a5-fb01-4bda-c786-976c487b2248",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification accuracy plot saved at /content/gdrive/MyDrive/stats/classification_accuracy.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>\n",
        "<font color=\"#FA8072\">5. Model Architecture</font>\n",
        "</h1>"
      ],
      "metadata": {
        "id": "Dqd4cNDvRfvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = [\"sentence_length\", \"word2vec\", \"word2vec_bigram\"]"
      ],
      "metadata": {
        "id": "iRv9I8m7QD-5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = {}\n",
        "for category in categories:\n",
        "    file_path = os.path.join(clean_data_dir, f\"{category}-cleaned.csv\")\n",
        "    if os.path.exists(file_path):\n",
        "        data[category] = pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "awv6APtRSe7N"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_transformer_model():\n",
        "    input_ids = Input(shape=(100,), dtype=tf.int32)\n",
        "    transformer = TFDistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "    output = transformer(input_ids)[0]\n",
        "    output = tf.keras.layers.GlobalAveragePooling1D()(output)\n",
        "    output = Dense(len(categories), activation=\"softmax\")(output)\n",
        "    model = Model(inputs=input_ids, outputs=output)\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "    return model"
      ],
      "metadata": {
        "id": "X5YsGdZVSuED"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_cnn_model():\n",
        "    input_shape = (100, len(features))\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(Conv1D(128, 5, activation='relu', input_shape=input_shape))\n",
        "    model.add(GlobalMaxPooling1D())\n",
        "    model.add(Dense(len(categories), activation='softmax'))\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "    return model"
      ],
      "metadata": {
        "id": "fUgPstuoSvZH"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_rnn_model():\n",
        "    input_shape = (100, len(features))\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(LSTM(128, input_shape=input_shape))\n",
        "    model.add(Dense(len(categories), activation='softmax'))\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "    return model"
      ],
      "metadata": {
        "id": "9AHM_v1lS-wQ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for category in categories:\n",
        "    train_data, test_data = train_test_split(data[category], test_size=0.2, random_state=42)\n",
        "    train_features = train_data[features].values\n",
        "    train_labels = train_data[\"label\"].values\n",
        "    test_features = test_data[features].values\n",
        "    test_labels = test_data[\"label\"].values\n",
        "    model = create_transformer_model()\n",
        "    model.fit(train_features, train_labels, epochs=10, batch_size=32, validation_split=0.2)\n",
        "    predictions = model.predict(test_features)\n",
        "    report = classification_report(test_labels, predictions)\n",
        "    print(report)\n",
        "    model.save(os.path.join(architecture_dir, f\"{category}_model\"))"
      ],
      "metadata": {
        "id": "fLdz_7s6TBEd"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>\n",
        "<font color=\"#FA8072\">6. Data Augmentation</font>\n",
        "</h1>"
      ],
      "metadata": {
        "id": "gnLWl7SgXIea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = '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'\n",
        "prompts = {\n",
        "    'category1': 'Prompt for category 1',\n",
        "    'category2': 'Prompt for category 2',\n",
        "    'category3': 'Prompt for category 3',\n",
        "    'category4': 'Prompt for category 4',\n",
        "    'category5': 'Prompt for category 5',\n",
        "    'category6': 'Prompt for category 6',\n",
        "}\n",
        "generated_data = {}\n",
        "for category, prompt in prompts.items():\n",
        "    response = openai.Completion.create(\n",
        "        engine='text-davinci-003',\n",
        "        prompt=prompt,\n",
        "        max_tokens=100,\n",
        "        temperature=0.7,\n",
        "        n=5,\n",
        "        stop=None\n",
        "    )\n",
        "    generated_data[category] = response.choices\n",
        "\n",
        "for category, generated_text in generated_data.items():\n",
        "    print(f\"Category: {category}\")\n",
        "    print(\"Original data: ...\")\n",
        "    print(\"Generated data: ...\")\n",
        "    print(\"Analysis: ...\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "LsgO47a6T0EO",
        "outputId": "849cfc40-a217-4e90-bb64-640babbc18c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-b6d124d02961>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mgenerated_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcategory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     response = openai.Completion.create(\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'text-davinci-003'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    151\u001b[0m         )\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         )\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             return (\n\u001b[0;32m--> 700\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    701\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    764\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m             )\n",
            "\u001b[0;31mRateLimitError\u001b[0m: You exceeded your current quota, please check your plan and billing details."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>\n",
        "<font color=\"#FA8072\">7. Rating by OpenAI</font>\n",
        "</h1>"
      ],
      "metadata": {
        "id": "Hbg1UJGbZcsX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Classify the following statement as Shot-Zero or Shot-Few:\"\n",
        "examples = [\n",
        "    ['Shot-Zero', 'This is a statement with very few shots.'],\n",
        "    ['Shot-Few', 'This statement has a reasonable number of shots.'],\n",
        "]"
      ],
      "metadata": {
        "id": "PlgK4zXqYTLJ"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "statements = [\n",
        "    'This is a statement with very few shots.',\n",
        "    'This statement has a reasonable number of shots.',\n",
        "    'The number of shots in this statement is quite high.',\n",
        "]"
      ],
      "metadata": {
        "id": "OsBA-NPFZwTV"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifications = openai.Completion.create(\n",
        "    engine=\"davinci\",\n",
        "    prompt=prompt,\n",
        "    logit_bias={\n",
        "        \"Shot-Zero\": -10,\n",
        "        \"Shot-Few\": 0,\n",
        "    },\n",
        "    max_tokens=2,\n",
        "    n=1,\n",
        ")\n",
        "for i, classification in enumerate(classifications.choices):\n",
        "    statement = statements[i]\n",
        "    label = classification.text.strip()\n",
        "    print(f\"Statement: {statement}\")\n",
        "    print(f\"Classification: {label}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "_NQ9me8dZ1AO",
        "outputId": "4669b6ec-c848-4ca7-da1d-ab7070102984",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-a3761876db1d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m classifications = openai.Completion.create(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"davinci\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     logit_bias={\n\u001b[1;32m      5\u001b[0m         \u001b[0;34m\"Shot-Zero\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    151\u001b[0m         )\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         )\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             return (\n\u001b[0;32m--> 700\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    701\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    764\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m             )\n",
            "\u001b[0;31mRateLimitError\u001b[0m: You exceeded your current quota, please check your plan and billing details."
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}